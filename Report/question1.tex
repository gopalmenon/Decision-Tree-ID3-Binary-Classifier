\section{Decision trees (35 points)}
\label{sec:q1}

\begin{enumerate}
\item 

\begin{enumerate}
\item $ (x_1 \lor x_2 ) \land x_3 $

\begin{forest}
  label L/.style={
    edge label={node[midway,left,font=\scriptsize]{#1}}
  },
  label R/.style={
    edge label={node[midway,right,font=\scriptsize]{#1}}
  },
  for tree={
    child anchor=north,
    for descendants={
      {edge=->}
    }
  },
  [$x_3$, decision, draw
    [$x_1$, decision, draw, label L=1,
      [1, rectangle, draw, label L=1, tier=bottom]
      [$x_2$, decision, draw, label L=0,
        [1, rectangle, draw, label L=1, tier=bottom]
        [0, rectangle, draw, label L=0, tier=bottom]
     ]
    ]
    [0, rectangle, draw, label R=0, tier=bottom]
  ]
\end{forest}
                                      
\item $(x_1 \land x_2)$  xor  $(\neg x_1 \lor x_3) $

\begin{forest}
  label L/.style={
    edge label={node[midway,left,font=\scriptsize]{#1}}
  },
  label R/.style={
    edge label={node[midway,right,font=\scriptsize]{#1}}
  },
  for tree={
    child anchor=north,
    for descendants={
      {edge=->}
    }
  },
  [$x_1$, decision, draw
    [1, rectangle, draw, label L=0, tier=bottom]
    [$x_3$, decision, draw, label L=1,
      [$x_2$, decision, draw, label L=0,
          [0, rectangle, draw, label L=0, tier=bottom]
          [1, rectangle, draw, label R=1, tier=bottom]
      ]
      [$x_2$, decision, draw, label R=1,
          [1, rectangle, draw, label L=0, tier=bottom]
          [0, rectangle, draw, label R=1, tier=bottom]
      ]
    ]
  ]
\end{forest}

\item The 2-of-3 function defined as follows: at least 2 of $\{x_1,
  x_2, x_3\}$ should be true for the output to be true.
  
  \begin{forest}
  label L/.style={
    edge label={node[midway,left,font=\scriptsize]{#1}}
  },
  label R/.style={
    edge label={node[midway,right,font=\scriptsize]{#1}}
  },
  for tree={
    child anchor=north,
    for descendants={
      {edge=->}
    }
  },
  [$x_1$, decision, draw
    [$x_2$, decision, draw, label L=1,
      [$x_3$, decision, draw, label L=1,
          [1, rectangle, draw, label L=1, tier=bottom]
          [1, rectangle, draw, label R=0, tier=bottom]
      ]
      [$x_3$, decision, draw, label R=0,
          [1, rectangle, draw, label L=1, tier=bottom]
          [0, rectangle, draw, label R=0, tier=bottom]
      ]
    ]
    [$x_2$, decision, draw, label R=0,
      [$x_3$, decision, draw, label L=1,
          [1, rectangle, draw, label L=1, tier=bottom]
          [0, rectangle, draw, label R=0, tier=bottom]
      ]
      [$x_3$, decision, draw, label R=0,
          [0, rectangle, draw, label L=1, tier=bottom]
          [0, rectangle, draw, label R=0, tier=bottom]
      ]
    ]
  ]
\end{forest}
  
  
\end{enumerate}

\item 

\begin{enumerate}
\item  ~[2 points] How many possible functions are there to map these four features to a Boolean decision? 

The number of possible rows in a truth table with all possible values of the four features will be\\

Number of types of Berries $\times$ Number of types of Balls $\times$ Number of colors $\times$ Number of Pok\'emon types

$= 2 \times 3 \times 3 \times 4$
$=72$\\

Each row in this truth table can have a value of $Yes$ or $No$ as a label for whether the Pok\'emon can be caught or not. The $Yes$ and $No$ can be represented as bits $1$ and $0$. Each possible function will map to one combination of these labels and will be of the form of a 72-bit binary number. So the total possible number of functions will be $2^{72}$, which is the count of all the possible 72-bit binary numbers.

\item  ~[2 points]  What is the entropy of the labels in this data? (When calculating entropy, The base of the logarithm should be 2.)

The entropy of a collection $S$ where the target label can take on $c$ different values is defined as \cite{Mitchell}
\begin{align*} 
Entropy(S) = \sum_{i=1}^c -p_i log_2 p_i
\end{align*}

where $p_i$ is the proportion of $S$ belonging to label $i$.

\begin{align*} 
Entropy(Pok \acute emon Data) &= -\frac{8}{16} log_2 \frac{8}{16} -\frac{8}{16} log_2 \frac{8}{16} \\
&= -\frac{1}{2} log_2 \frac{1}{2} -\frac{1}{2} log_2 \frac{1}{2}\\
&= -log_2 \frac{1}{2} \\
&= 1
\end{align*}

\item  ~[8 points] Calculate information gain for four features respectively. Keep 3 significant digits.  

\begin{align*} 
Values(Berry) &= Yes , No\\
S &= \left [ 8+, 8- \right ] \\
S_{Yes} &= \left [ 6+, 1- \right ] \\
S_{No} &= \left [ 2+, 7- \right ] \\
Gain(S , Berry) &= Entropy(S) - \frac{7}{16} Entropy(S_{Yes}) - \frac{9}{16} Entropy(S_{No})\\
&= 1 - \frac{7}{16} \times 0.5917 - \frac{9}{16} \times 0.7642\\
&= 0.311
\end{align*}
\begin{align*} 
Values(Ball) &= Pok \acute e , Great , Ultra\\
S_{Pok \acute e} &= \left [ 1+, 5- \right ] \\
S_{Great} &= \left [ 4+, 3- \right ] \\
S_{Ultra} &= \left [ 3+, 0- \right ] \\
Gain(S , Ball) &= Entropy(S) - \frac{6}{16} Entropy(S_{Pok \acute e}) - \frac{7}{16} Entropy(S_{Great}) - \frac{3}{16} Entropy(S_{Ultra})\\
&= 1 - \frac{6}{16} \times 0.65 - \frac{7}{16} \times 0.9852 -\frac{3}{16} \times 0\\
&= 0.3252
\end{align*}
\begin{align*} 
Values(Color) &= Green , Yellow , Red\\
S_{Green} &= \left [ 2+, 1- \right ] \\
S_{Yellow} &= \left [ 3+, 4- \right ] \\
S_{Red} &= \left [ 3+, 3- \right ] \\
Gain(S , Color) &= Entropy(S) - \frac{3}{16} Entropy(S_{Green}) - \frac{7}{16} Entropy(S_{Yellow}) \\ 
&- \frac{6}{16} Entropy(S_{Red})\\
&= 1 - \frac{3}{16} \times 0.9183 - \frac{7}{16} \times 0.9852 -\frac{6}{16} \times 1\\
&= 0.0218
\end{align*}
\begin{align*} 
Values(Type) &= Normal , Water , Flying, Psychic\\
S_{Normal} &= \left [ 3+, 3- \right ] \\
S_{Water} &= \left [ 2+, 2- \right ] \\
S_{Flying} &= \left [ 3+, 1- \right ] \\
S_{Psychic} &= \left [ 0+, 2- \right ] \\
\end{align*}
\begin{align*} 
Gain(S , Type) &= Entropy(S) - \frac{6}{16} Entropy(S_{Normal}) - \frac{4}{16} Entropy(S_{Water}) \\ 
&- \frac{4}{16} Entropy(S_{Flying}) - \frac{2}{16} Entropy(S_{Psychic})\\
&= 1 - \frac{6}{16} \times 1 - \frac{4}{16} \times 1 -\frac{4}{16} \times 0.8113 - \frac{2}{16} \times 0\\
&= 0.172
\end{align*}
\item  ~[3 points] According to your results, using ID3 algorithm which attribute should be root for the decision tree?

The root should be feature $Ball$, since it has the largest entropy gain.

\item   ~[4 points] Construct a decision with the root you selected in the previous question. You do not have to use the ID3 algorithm here, you can show any tree with the chosen root.

 \begin{forest}
  label L/.style={
    edge label={node[midway,left,font=\scriptsize]{#1}}
  },
  label R/.style={
    edge label={node[midway,right,font=\scriptsize]{#1}}
  },
  for tree={
    child anchor=north,
    for descendants={
      {edge=->}
    }
  },
  [$Ball$, decision, draw
    [$Berry$, decision, draw, label L=Pok\'e,
      [$Color$, decision, draw, label L=Yes,
          [Yes, rectangle, draw, label L=Green, tier=bottom]
          [Yes, rectangle, draw, label L=Yellow, tier=bottom]
          [No, rectangle, draw, label R=Red, tier=bottom]
      ]
      [No, rectangle, draw, label L=No, tier=bottom]
    ]
    [Yes, rectangle, draw, label R=Ultra, tier=bottom]
    [$Berry$, decision, draw, label L=Great,
        [Yes, rectangle, draw, label L=Yes, tier=bottom]
        [No, rectangle, draw, label R=No, tier=bottom]
    ]
    ]
  ]
\end{forest}


\item   ~[2 points] Using your decision tree to predict label in the test set in the table below, what is your label for the each example? What is your accuracy?

Only one out of three was classified correctly. So accuracy is low.

\begin{table}[H]
\centering
\begin{tabular}{| c c c c | c |c|}
\hline
Berry& Ball & Color & Type & Caught & Prediction\\
\hline
Yes & Great & Yellow & Psychic & Yes & Yes\\
Yes & Pok\'e & Green & Flying & No & Yes\\
No & Ultra & Red & Water & No & Yes\\
\hline
\end{tabular}
\end{table}

\item   ~[1 points]  Do you think it is a good idea to use decision tree in this Pok\'emon Go problem?

It is not a good idea to use a decision tree for this particular Pok\'emon Go problem. The training data looks to be not enough for a learning algorithm and the test data seems to be adversarial. A decision tree may perform better with a larger test data set.

\begin{table}[H]
\centering
\begin{tabular}{| c c c c | c |}
\hline
Berry& Ball & Color & Type & Caught\\
\hline
Yes & Great & Yellow & Psychic & Yes \\
Yes & Pok\'e & Green & Flying & No \\
No & Ultra & Red & Water & No \\
\hline
\end{tabular}
\caption{Test set for Pok\'emon Go}
\end{table}


\end{enumerate}
\item Recall that in the ID3 algorithm, we want to identify the best
  attribute that splits the examples that are relatively pure in one
  label. Apart from entropy, which you used in the previous question,
  there are other methods to measure impurity. One such impurity
  measure is the Gini measure, that is used in the CART family of
  algorithms. If there are $k$ possible outcomes
  $1,\cdots, i, \cdots, k$, each with a probability
  $p_1, \cdots, p_i, \cdots, p_k$ of occurring, the Gini measure is
  defined as:
\begin{equation*}
Gini(p_1, \cdots, p_k) =   1 - \sum_{i=1}^k p_i^2
\end{equation*}
The Gini measure can be used to replace entropy in the definition of information gain to pick the best attribute.
\begin{enumerate}
\item ~[4 points] Using the Gini measure, calculate the information gain for the four features respectively. Use 3 significant digits. 
\item ~[3 points] According to your results in the last question,
  which attribute should be the root for the decision tree? Do these
  two measures (entropy and Gini) lead to the same tree?
\end{enumerate}
\end{enumerate}





