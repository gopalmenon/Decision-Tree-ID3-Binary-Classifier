\section{Experiments}
\label{sec:q1}

\subsection*{Setting A [25 points]}

\begin{enumerate}
\item ~[10 points] \textbf{Implementation}

  \begin{enumerate}
  \item ~[4 points] I used the Pok\'emon training and test data to verify that the decision tree had been coded properly. The decision tree was coded based on the algorithm in Tom Mitchell\'s book \cite{Mitchell}. A random number generator was used when I needed to break a tie in the selection of the most common value of an attribute. To check the accuracy of the classifier I used the statistical metrics for Precsion, Recall, Accuracy and F1 Score. The Java Collection library classes were used as they made programming a little easier. The Tree was implemented using a class hierarchy of generic node as the parent class of a leaf node and an internal node. The child nodes for an internal node were represented using a collection of nodes.
    
  \item ~[2 points]  There was no error when testing on the training data.
    
  \item ~[5 points] There was no error when testing on the testing data.
    
  \item ~[1 points] The maximum tree depth was 3.
  \end{enumerate}

\item ~[15 points] \textbf{Limiting Depth}

  \begin{enumerate}
  \item ~[10 points] 
    \begin{center}
    \begin{tabular}{c|c|c}
      $Depth$ & $Average \; Accuracy$ & $Standard Deviation$\\ \hline
      1 & 0.983 & 0.037\\
      2 &  0.997 & 0.007\\
       3 & 1.0 & 0.0\\
       4 & 1.0 & 0.0\\
       5 & 1.0 & 0.0\\
       10 &1.0 & 0.0\\
      15 & 1.0 & 0.0\\
      20 & 1.0 & 0.0
    \end{tabular}
  \end{center}
    
  \item ~[5 points]  A maximum depth of 3 was specified for the decision tree since it was the smallest tree that resulted in an average accuracy of 1.0 with 6-fold cross validation. With training and test data and a depth of 3, the accuracy obtained was 1.0.
  \end{enumerate}

\end{enumerate}

\subsection*{Setting B [25 points]}

\begin{enumerate}
\item ~[10 points] \textbf{Experiments}
  
  For this problem, you will be using the data found in the \texttt{SettingB} folder. This folder contains the two files, \texttt{SettingB/training.data} and \texttt{SettingB/test.data}. In this setting you will be training your algorithm on the training file (\texttt{SettingB/training.data}). Remember that you should not look at or use your testing file until your algorithm is complete. You are not limiting the depth of your tree in this section. 
  
  \begin{enumerate}
    
  \item ~[2 points]  Report the error of your decision tree on the \texttt{SettingB/training.data} file. 
    
  \item ~[2 points] Report the error of your decision tree on the \texttt{SettingB/test.data} file. 
    
  \item ~[2 points] Report the error of your decision tree on the \texttt{SettingA/training.data} file. 
    
  \item ~[2 points] Report the error of your decision tree on the \texttt{SettingA/test.data} file. 
    
  \item ~[1 points] Report the maximum depth of your decision tree. 
  \end{enumerate}
  
\item ~[15 points] \textbf{Limiting Depth}
  
  In this section you will be using 6-fold cross-validation in order to limit the depth of your decision tree, effectively pruning the tree to avoid overfitting. You will be using the 6 cross-validation files for this section, titled \texttt{SettingB/CVSplits/training\_0X.data} where \texttt{X} is a number between 0 and 5 (inclusive). 
  
  \begin{enumerate}
  \item ~[10 points] Run 6-fold cross-validation using the specified files. Experiment with depths in the set $\{1,2,3,4,5,10,15,20\}$, reporting the cross-validation accuracy and standard deviation for each depth. Explicity specify which depth should be chosen as the best, and explain why. 
    
  \item ~[5 points]  Using the depth with the greatest cross-validation accuracy from your experiments: train your decision tree on the \texttt{SettingB/training.data} file. Report the accuracy of your decision tree on the \texttt{SettingB/test.data} file. 	
  \end{enumerate}
  
\end{enumerate}

\subsection*{Setting C (CS 6350 Students) [20 points]}

In this setting, you are investigating what effect missing features have on a decision tree, and exploring which approach is most effective in dealing with missing features. More specifically, you will be trying:
\begin{itemize}
\item \textbf{Method 1:} ~Setting the missing feature as the majority feature value.
\item \textbf{Method 2:} ~Setting the missing feature as the majority value of that label. 
\item \textbf{Method 3:} ~Treating the missing feature as a \textit{special} feature.
\end{itemize}
The missing feature is represented by a \texttt{?} character. In order to determine which method is the best, you will be using 6-fold cross-validation, with the files being titled\\ \texttt{SettingC/CVSplits/training\_0X.data} where \texttt{X} is a number between 0 and 5 (inclusive). These files, along with \texttt{SettingC/training.data} and \texttt{SettingC/test.data} can be found in the \texttt{SettingC} folder. 

\begin{enumerate}

  
\item ~[5 points] Update your decision tree implementation to have functionality to deal with missing features. Describe your approach/any choices you had to make in this implementation. 
  
\item ~[10 points] Perform 6-fold cross-validation on each of the 3 methods described above. Report the accuracy for each method and the standard deviation. 
  
\item ~[5 points] Using the best method selected from your experiments, train your decision tree on \texttt{SettingC/training.data}, and report the accuracy of your tree on \texttt{SettingC/test.data}. 
\end{enumerate}





